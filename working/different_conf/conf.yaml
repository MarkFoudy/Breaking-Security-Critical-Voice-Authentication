inputs: inputs/attack_inputs.txt 
input_dir: "asvspoofWavs"
lengths: inputs/lengths

sr: 16000
length: 12000 #48000
nr_nfft: 400
optimization_itrs: 450 
prop_decrease: 0.7

shadow: dev
target: train #dev, train

loss: null 

discriminator_wb: 
    args: 
        cm: #ignored for ADVSR
            selector: [lcnnFull] #[lcnnFull] #[lcnnFull] #one of [SSNet, SENet, lcnnFull, lcnnHalf, rawGAT_st, mcg, mlcg, Res2Net, AIR, comparative, darts]
            lambda: 3 #ignored for ADVSR, ADVCM
        asv: #ignored for ADVCM
            selector: [mfcc] #[mfcc] #[mfcc] #one of [mfcc, lps]
            lambda: 1 #ignored for ADVSR, ADVCM
        

discriminators:
    args:
        cm: #ignored for ADVSR
            selector: [SSNet, lcnnFull, rawGAT_st, mcg, mlcg, Res2Net, AIR, darts, comparative, RawDarts, AASSIST, AASSIST-L, AIR_AM] 
        asv: #ignored for ADVCM
            selector: [mfcc] #one of [mfcc, lps]

TIME_DOMAIN_ATTACK:
    epsilon: 0.001 #0.001 
    max_iter: 30

FFT_Attack:
    epsilon: 0.015
    max_iter: 10

STFT_Attack:
    epsilon: 0.0005 #0.05
    max_iter: 30 #15
    nfft: 400
    window: 'bartlett_window' #kaiser_window hann_window hamming_window bartlett_window blackman_window
    hop_length: 160
    win_length: 400  

auto_pgd:
    #norm: 2
    eps: 0.003 #8/255
    eps_step: 0.0003
    
    targeted: true
    nb_random_init: 1
    batch_size: 1
    #loss_type

carlini:
    targeted: true
    #max_iter: 500
    confidence: 5.0 #>= 0.0. Confidence of adversarial examples: a higher value produces examples that are farther away,
    learning_rate: 0.01 #The initial learning rate for the attack algorithm. Smaller values produce better results
    #binary_search_steps: 10
    #max_halving: 10 
    #max_doubling: 10 

boundary:
    batch_size: 1
    targeted: false
    max_iter: 100
    epsilon: 1 #lower values caused the model never to converge. 0.01. Initial step size for the step towards the target
    delta: 1 #same. 0.01. Initial step size for orthogonal step
    num_trial: 25 #Maximum number of trials for initial generation of adversarial examples
    sample_size: 1 #best otherwise we have to change code. Not supported
    init_size: 10 #matches sample_size

bb:
    #norm : inf #The norm of the adversarial perturbation. Possible values: "inf", np.inf, 1 or 2
    batch_size: 1
    init_size: 25 #Maximum number of random search steps to find initial adversarial example.
    overshoot: 1.
    steps: 20
    binary_search_steps: 10 #maybe change this. works for 1
    lr: 1.e-7

